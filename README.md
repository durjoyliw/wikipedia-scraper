# ğŸŒ Wikipedia Disaster Data Scraper

[![Node.js](https://img.shields.io/badge/Node.js-16%2B-green.svg)](https://nodejs.org/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![JavaScript](https://img.shields.io/badge/JavaScript-ES6%2B-yellow.svg)](https://developer.mozilla.org/en-US/docs/Web/JavaScript)
[![GitHub Stars](https://img.shields.io/github/stars/durjoyliw/wikipedia-scraper?style=social)](https://github.com/durjoyliw/wikipedia-scraper/stargazers)

> A comprehensive toolkit for collecting, analyzing, and packaging disaster data from Wikipedia with professional reporting capabilities.

## ğŸ“Š Project Results

ğŸ¯ **Successfully Collected:**
- **200+** disaster events documented
- **25,217** external sources gathered
- **768 years** of historical data (1256-2024)
- **100%** successful extraction rate

## ğŸš€ Features

### ğŸ”§ **Four Powerful Tools**
- **ğŸ“„ Single Page Scraper** - Extract structured data from any Wikipedia list page
- **ğŸ•·ï¸ Mass Crawler** - Automatically crawl hundreds of linked pages for detailed information
- **ğŸ“ˆ Analysis Engine** - Generate comprehensive statistical reports and insights
- **ğŸ“¦ Professional Packager** - Create client-ready deliverables with executive summaries

### âš¡ **Key Capabilities**
- âœ… **Smart Reference Resolution** - Extracts external sources and citations
- âœ… **Respectful Crawling** - Built-in delays to respect Wikipedia's servers
- âœ… **Professional Documentation** - Auto-generates executive summaries and user guides
- âœ… **Multiple Export Formats** - JSON, CSV, and formatted reports
- âœ… **Error Recovery** - Robust error handling prevents data loss
- âœ… **Scalable Architecture** - Handle 1000+ pages with current framework

## ğŸ› ï¸ Installation

### Prerequisites
- [Node.js](https://nodejs.org/) (version 16 or higher)
- npm (comes with Node.js)

### Quick Setup
```bash
# Clone the repository
git clone https://github.com/durjoyliw/wikipedia-scraper.git

# Navigate to project directory
cd wikipedia-scraper

# Install dependencies
npm install
```

## ğŸ“– Usage

### Category-Based Workflow

#### 1ï¸âƒ£ **Single Page Scraping**
Perfect for targeted data collection from one Wikipedia page.

```bash
# Edit the URL in wikiScraper1.js, then run:
node wikiScraper1.js
```

**Use Cases:**
- Extract data from specific disaster lists
- Test scraper on new page types
- Quick data collection for small projects

#### 2ï¸âƒ£ **Mass Crawling**
Collect detailed information from hundreds of individual pages.

```bash
# Must run step 1 first, then:
node auto_crawler.js
```

**Use Cases:**
- Comprehensive research datasets
- Building large-scale databases
- Academic research projects

#### 3ï¸âƒ£ **Analysis & Reporting**
Generate professional reports and client packages.

```bash
# Generate statistical analysis
node generate_reports.js

# Create professional delivery package
node create_client_package.js
```

**Use Cases:**
- Client deliverables
- Stakeholder presentations
- Research publication packages

### Configuration

**To change target Wikipedia page:**
```javascript
// In wikiScraper1.js, modify:
const testUrl = "https://en.wikipedia.org/wiki/Your_Target_Page";
```

**To adjust crawling parameters:**
```javascript
// In auto_crawler.js, modify:
crawler.maxPages = 50;    // Number of pages to crawl
crawler.delay = 3000;     // Delay between requests (ms)
```

## ğŸ“ Output Structure

### Single Page Scraping
```
ğŸ“„ PageName_2025-07-16.json      # Complete structured data
ğŸ“Š PageName_2025-07-16.csv       # Excel-ready tables
ğŸ“‹ PageName_2025-07-16_report.txt # Summary report
```

### Mass Crawling
```
ğŸ“ crawled_pages/
  â”œâ”€â”€ ğŸ“„ Black_Death.json           # Individual event files
  â”œâ”€â”€ ğŸ“„ Great_Famine_Ireland.json  # (200+ detailed files)
  â”œâ”€â”€ ğŸ“Š MASTER_INDEX.csv           # Complete catalog
  â””â”€â”€ ğŸ“‹ MASTER_INDEX.json          # Technical format
```

### Analysis & Reports
```
ğŸ“ ANALYSIS_REPORTS/
  â”œâ”€â”€ ğŸ“‹ ANALYSIS_REPORT.txt        # Comprehensive insights
  â”œâ”€â”€ ğŸ“Š DISASTER_TYPES.csv         # Category breakdown
  â”œâ”€â”€ ğŸ“ˆ TIMELINE_DATA.csv          # Historical timeline
  â””â”€â”€ ğŸŒ LOCATION_ANALYSIS.csv      # Geographic distribution

ğŸ“ CLIENT_DELIVERY_PACKAGE/
  â”œâ”€â”€ ğŸ“‹ EXECUTIVE_SUMMARY.txt      # Professional overview
  â”œâ”€â”€ ğŸ“– USER_GUIDE.txt             # Data access guide
  â”œâ”€â”€ ğŸ“Š SAMPLE_EVENTS.csv          # Quick preview
  â””â”€â”€ ğŸ“ˆ MASTER_INDEX.csv           # Complete catalog
```

## ğŸ“ˆ Example Results

### Data Quality Metrics
- **Average Sources per Event:** 126 verified references
- **Geographic Coverage:** 15+ countries and regions
- **Temporal Span:** Medieval period to present day
- **Source Types:** Academic, governmental, news, and archival

### Disaster Categories Analyzed
- ğŸ¦  Pandemics & Disease Outbreaks
- ğŸŒŠ Natural Disasters
- ğŸ¥€ Famines & Food Crises
- ğŸ’¥ Industrial & Transport Accidents
- âš”ï¸ Terrorist Incidents & Violence

## ğŸ” Use Cases

### ğŸ“š **Academic Research**
- Historical disaster pattern analysis
- Comparative impact studies
- Source verification for publications
- Temporal trend analysis

### ğŸ¢ **Business Intelligence**
- Risk assessment and planning
- Historical precedent research
- Content creation with verified facts
- Market analysis for insurance/planning sectors

### ğŸ›ï¸ **Government & NGOs**
- Disaster preparedness planning
- Historical context for policy making
- Public education material development
- Emergency response benchmarking

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### Development Setup
```bash
# Fork the repository
# Clone your fork
git clone https://github.com/YOUR-USERNAME/wikipedia-scraper.git

# Create a feature branch
git checkout -b feature/amazing-feature

# Make your changes and commit
git commit -m 'Add some amazing feature'

# Push to the branch
git push origin feature/amazing-feature

# Open a Pull Request
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Wikipedia** for providing comprehensive, well-structured disaster information
- **Node.js Community** for excellent libraries (axios, cheerio)
- **Open Source Contributors** who make projects like this possible

## ğŸ“ Contact

**Durjoy Liw** - [GitHub](https://github.com/durjoyliw)

Project Link: [https://github.com/durjoyliw/wikipedia-scraper](https://github.com/durjoyliw/wikipedia-scraper)

---

â­ **Star this repository if you found it helpful!** â­
